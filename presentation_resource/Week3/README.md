# 워드 임베딩

워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환하는 것이다. 

> 희소 표현 vs 밀집 표현


1) 희소 표현(Sparse Representation)

  - 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현이라고 함

   ex) 원-핫 인코딩 방법d으로 생성된 원-핫 벡터들 = 희소 벡터(sparse vector) 
 -표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법.
 
 - 공간적 낭비를 불러일으킴
 - 단어의 의미를 담지 못함

2) 밀집 표현(Dense Representation)

- 벡터의 차원을 단어 집합의 크기로 상정X

- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤.
-> 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 됨.

Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128
  

3) 워드 임베딩(Word Embedding)

![image](https://user-images.githubusercontent.com/61388801/130607178-3f4d0e24-b119-45dc-80cf-552e0030dea3.png)


) 워드투벡터

) 글로브

) 패스트텍스트(FastText)

# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)

> 케라스 임베딩 vs 사전 훈련된 워드 임베딩

자연어 처리를 힐 때 '갖고 있는 훈련 데이터의 단어'들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있다. 
케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현한다.

그런데 위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 
''이미 미리 훈련된 임베딩 벡터''를 불러오는 방법을 사용하는 경우도 있는데, 이는 위의 경우와 대비되는 경우이다.

1) 케라스 임베딩 층(Keras Embedding layer)
케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공한다.
이때 이 Embedding()은 인공 신경만 구조 관점에서 임베딩 층을 구현한다.

- 임베딩 층은 룩업 테이블이다.

임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 한다.

어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터

임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 
-> 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다

이때 정수를 '밀집 벡터'또는 '임베딩 벡터'로 메핑한다는 것은 

특정 단어와 맵핑되는 정수를/ 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 할 수 있다.ㄷ
이때 이 테이블은 '단어 집합의 크기'만큼의 해을 가지므로, 모든 단어는 고유한 임베딩 벡터를 가진다.

![image](https://user-images.githubusercontent.com/61388801/130608482-87eeef8c-fb55-4cf4-ac9c-c6513122c507.png)

- 임베딩 층 사용하기


2. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기
