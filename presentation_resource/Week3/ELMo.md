# Deep contextualized word representations(ELMo) ë…¼ë¬¸ ë¦¬ë·°

ELMoëŠ” Embeddings from Language Modelì˜ ì•½ìë¡œ í•´ì„í•˜ë©´ 'ì–¸ì–´ ëª¨ë¸ë¡œ í•˜ëŠ” ì„ë² ë”©'ì´ë‹¤.
ELMoì˜ ê°€ì¥ í° íŠ¹ì§•ì€ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(Pre-trained language model)ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤.

ì¦‰, ì—˜ëª¨ëŠ” ì‚¬ì „í›ˆë ¨ê³¼ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ëŠ” ë¬¸ë§¥ ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ì´ë‹¤.

## Introduction

ì´ representationì€ (ë¬¸ì¥ ë‚´) ê° tokenì´ ì „ì²´ ì…ë ¥ sequenceì˜ í•¨ìˆ˜ì¸ representationë¥¼ í• ë‹¹ë°›ëŠ”ë‹¤ëŠ” ì ì—ì„œ ì „í†µì ì¸ ë‹¨ì–´ embeddingê³¼ ë‹¤ë¥´ë‹¤.

ì´ë¥¼ ìœ„í•´ ì´ì–´ë¶™ì—¬ì§„ language model(LM)ë¡œ í•™ìŠµëœ bidirectional LSTM(biLM)ë¡œë¶€í„° ì–»ì€ vectorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë˜í•œ, lstmì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¡´ì˜ ë°©ë²•ë¡ ê³¼ëŠ” ë‹¬ë¦¬, ELMoëŠ” lstmì˜ ëª¨ë“  ë‚´ë¶€ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•´ì„œ ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì— ë”ìš± ë§ì€ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
- higher-level LSTM : ë¬¸ë§¥ì„ ë°˜ì˜í•œ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì˜ í‘œí˜„
- lower-level LSTM : ë‹¨ì–´ì˜ ë¬¸ë²•ì ì¸ ì¸¡ë©´ì„ ì˜ í‘œí˜„


![image](https://user-images.githubusercontent.com/61388801/130611787-b2567202-71d5-49c7-8f2b-0985690d51e2.png)

ì¦‰, ElMoëŠ” ìœ„ì˜ ì˜ˆì‹œì™€ ê°™ì´, 'ë‹¨ì–´ì˜ ë¬¸ë§¥, ì˜ë¯¸, ë‹¤ì–‘ì„±'ì„ ê³ ë ¤í•˜ê³ , ì´ì „ ëª¨ë¸ê³¼ ë‹¬ë¦¬ ì´ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

## ELMo

ELMoëŠ”

- ì „ì²´ ë¬¸ì¥ì„ inputìœ¼ë¡œ ë°›ê³ , ê·¸ì— ëŒ€í•œ ê° ë‹¨ì–´ë“¤ì˜ representation ìƒì‚°
- Character convolutionë¡œë¶€í„° ì–»ì€ biLMì˜ ê°€ì¥ ìœ„ 2ê°œ layerì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°
- í° ì‚¬ì´ì¦ˆë¡œ biLM pretrain ì‹œì¼°ì„ ë•Œ semi-supervised learning ê°€ëŠ¥
- ì‰½ê²Œ ë‹¤ë¥¸ ëª¨ë¸ì— ë¶™ì¼ ìˆ˜ ìˆìŒ

### 3-1) Bidirectional language models

biLM = forward language model + backward language model

Nê°œì˜ token (t1,t_2, â€¦,t_N)ì´ ìˆë‹¤ê³  í•˜ë©´

> forward language model

 (t_1,t_2, â€¦,t(k-1))ì´ ì£¼ì–´ì¡Œì„ ë•Œ token t_kê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒ

![image](https://user-images.githubusercontent.com/61388801/130612348-46c03e16-f541-4298-8f2b-9b49ef3f1df9.png)


> backward language model

(ğ‘¡(ğ‘˜+1),ğ‘¡(ğ‘˜+2), â€¦,ğ‘¡ğ‘)ì´ ì£¼ì–´ì¡Œì„ ë•Œ token ğ‘¡ğ‘˜ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°

![image](https://user-images.githubusercontent.com/61388801/130613011-c34471a6-fab3-44d3-a8af-6cf6d8cee200.png)

biLM = forward language model + backward language model ì´ì—ˆìœ¼ë‹ˆ,

ë‘ ë°©í–¥ì˜ log likelihoodë¥¼ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

![image](https://user-images.githubusercontent.com/61388801/130612823-ca94e1b4-113d-472b-99c8-59af3f40f345.png)

ì´ë•Œ (Î˜xëŠ” token representation, Î˜sëŠ” Softmax layer)ì´ë©°, ì´ ë‘˜ì€ parameterì™€ëŠ” ë‹¤ë¥´ê²Œ ê³ ì •ëœë‹¤.

### 3-2) ELMo



