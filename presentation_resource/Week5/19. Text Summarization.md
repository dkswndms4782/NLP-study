**<i>✨Text Summarization✨</i>**
========
- 텍스트 요약: 상대적으로 큰 원문을 **핵심 내용만** 간추려서 상대적으로 작은 **요약문**으로 변환하는 것.

<br>
<br>
<br>
========
<br>

# **<i>1. Text Summarization with Attention</i>**
## <i>Text Summarization</i>
- 추출적 요약(extractive summarization)과 추상적 요약(abstractive summarization)

<br>

**1. 추출적 요약(extractive summarization)**
- 원문에서 중요한 핵심 문장 또는 단어구를 몇 개 뽑아서 이들로 구성된 요약문을 만드는 방법
- (ex) TextRank
    - 단점: 이미 존재하는 문장이나 단어구로만 구성하므로 모델의 언어 표현 능력이 제한됨

<br>

**2. 추상적 요약(abstractive summarization)**
- 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법
- (ex) seq2seq
    - 지도 학습이기 때문에 '실제 요약문' 이라는 레이블 데이터가 있어야 한다.

<br>
========
<br>

## <i>About Amazon Review Data</i>
- [데이터 다운로드](https://www.kaggle.com/snap/amazon-fine-food-reviews)
- [Colab Notebook](https://colab.research.google.com/drive/187O4SGsupjGuBOv8MatZ_VEcT4m_Mog2#scrollTo=k_3mJ2AyeDMf)

<br><br><br>

## <i>Designing and Training Seq2Seq + Attention Model</i>
- [Colab Notebook](https://colab.research.google.com/drive/187O4SGsupjGuBOv8MatZ_VEcT4m_Mog2#scrollTo=nJgh3nU7uA6z)


<br><br><br>

## <i>Testing Model</i>
- [Colab Notebook](https://colab.research.google.com/drive/187O4SGsupjGuBOv8MatZ_VEcT4m_Mog2#scrollTo=yteSzk2FyBSK)

<br><br><br><br>
======
<br>

# **<i>TextRank Based on Sentence Embedding</i>**
- extractive summarization

## <i>TextRank</i>
- 페이지랭크 알고리즘: 검색 엔진에서 웹 페이지의 순위를 정하기 위해 사용되던 알고리즘
- [텍스트랭크](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) 알고리즘: 페이지랭크를 기반으로 한 텍스트 요약 알고리즘.

<br>

- 그래프 기반의 랭킹 모델
    - 각 정점의 정보만을 고려하지 않고 전체 그래프의 글로벌 정보를 재귀적으로 계산하여 정점의 중요도를 결정.
1. 텍스트를 식별하고 점으로 추가한다.
2. 텍스트 간 관계를 식별하고 엣지를 그린다.
    - directed, undirected, weighted, unweighted가 될 수 있다.
3. 수렴할 때까지 그래프 기반 랭킹 알고리즘 반복
4. 점수에 따라 정점 정렬

<br>

**키워드 추출**
- co-occurence 관계: N 범위를 기준으로 연결되는 단어의 수. 두 단어의 간격이 최대 범위 N 사이에서 연결되며 N은 2~10 사이의 값.

```python
# N=2로 하여 co-occurence를 구해보자!
text = ["Mon Tue Sat Wed Sat Thu Fri Fri Sat Sun"]

# Wed를 기준으로 보자!
Tue, Sat,   Sat, Thu

# Sat가 여러 번 나왔으니까 Sat를 기준으로 보자!
Mon, Tue,   Wed, Sat # Mon*1, Tue*1, Wed*1, Sat*1
Sat, Wed,   Thu, Fri # Sat*1, Wed*1, Thu*1, Fri*1
Fri, Fri,   Sun # Fri*2, Sun*1

# 중복을 제거하면? (=Sat 횟수는 카운트 X)
Sat: Mon*1, Tue*1, Wed*2, Thu*1, Fri*3, Sun*1

# 알고리즘을 반복한 결과를 표로 나타내보자!
```

|Table|Fri|Mon|Sat|Sun|Thu|Tue|Wed|
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|Fri|1|0|3|1|2|0|0|
|Mon|0|0|1|0|0|1|0|
|Sat|3|1|1|1|1|1|2|
|Sun|1|0|1|0|0|0|0|
|Thu|2|0|1|0|0|0|1|
|Tue|0|1|1|0|0|0|1|
|Wed|0|0|2|0|1|1|0|

<br>

![In Paper](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fojxi6%2FbtqQZwUSPzG%2F0tdTrgzPQjnJoA3RI0sgP0%2Fimg.png)

<br>

**문장 추출**
- co-occurence 관계는 문장 추출에서 쓸 수 없다.
- 두 문장에 공통으로 포함되는 단어의 개수를 각 문장의 단어 개수의 log 값의 합으로 나눈 값
<img src="https://render.githubusercontent.com/render/math?math=Similarity(S_i, S_j) = \frac{| \{ w_k | w_k \in S_i, w_k \in S_j \} |}{log(|S_i|) + log(|S_j|)}">
- 문장 <img src="https://render.githubusercontent.com/render/math?math=S_i">는 <img src="https://render.githubusercontent.com/render/math?math=N_i"> 개의 단어로 표현된다. <img src="https://render.githubusercontent.com/render/math?math=S_i = w_{1}^{i}, w_{2}^{i}, ..., w_{N_i}^{i}">

<br><br><br>

## <i>Pre-trained Embedding</i>

- [Colab Notebook](https://colab.research.google.com/drive/1zzj1jolUXIvh8tc1PQ87ihXIhUJpBK_A#scrollTo=n-3st1sIH_pL)

### **GloVe를 복습해보자!**
- [자세한 설명](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)

- 카운트 기반과 예측 기반을 모두 사용하는 워드 임베딩 방법론
 
<br>

- 동시 등장 확률(Co-occurence Probability): 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률.
    - $P(k|i)$ ($k$: context word, $i$: center word)
    - **임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만듦**

<br>

### **FastText를 복습해보자!**
- 각 단어는 글자 단위 N-gram의 구성으로 취급된다.
```python
# 단어 apple을 N=3으로 분리해보자!
<ap, app, ppl, ple, le>, <apple>

# 단어 apple을 N=range(3, 7)로 분리해보자!
<ap, app, ppl, ple, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>
```
- 데이터셋이 충분하면 OOV에 대해서도 다른 단어와의 유사도 계산 가능
- 단어가 희귀하더라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치면, 꽤 괜찮은 결과값(노이즈에 robust)

<br>

### **Word2Vec을 복습해보자!**
- CBOW(Continuous Bag of Words): context words로 center word 예측
- Skp-Gram: center word로 context words 예측
- NNLM보다 학습 속도가 훨씬 더 빠르다.

<br><br><br>

## <i>Sentence Embedding</i>
- 문장 벡터 얻기: 문장에 존재하는 단어 벡터들의 평균을 구함으로써 얻을 수 있다.
[Colab NoteBook](https://colab.research.google.com/drive/1zzj1jolUXIvh8tc1PQ87ihXIhUJpBK_A#scrollTo=VOebgN93a8ww)

<br><br><br>

## <i>Text Summarization with TextRank</i>
[Colab NoteBook](https://colab.research.google.com/drive/1zzj1jolUXIvh8tc1PQ87ihXIhUJpBK_A#scrollTo=AlHJaRn_a2Nb)
