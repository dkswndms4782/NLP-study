# 1.RNN(Recurrent Neural Network, 순환 신경망)
  <img src="https://user-images.githubusercontent.com/59716219/129435017-52dee5fd-fc8f-44c0-89c3-d5adeaf0394e.png" width="500" height="250">

+ `RNN`은 `은닉층의 노드`에서 나온 결과값을 다시 은닉층 노드의 다음 계산의 입력으로 보내는 `재귀적 활동`을 한다.
+ 이때 은닉층의 노드를 `메모리 셀, RNN 셀`이라고 한다.(위 그림에서 초록색 노드)
+ 은닉층의 메모리셀은 각각의 시점(time step)에서 재귀적 활동을 한다. 현재 시점이 t라면, 메모리 셀이 다음지점 t+1의 자기자신에게 보내는 값을 `은닉 상태(hidden state)` 라고 한다.
+ x: 입력층의 입력 벡터, y: 출력층의 출력 벡터
+ RNN은 각 층을 벡터 단위로 가정하고 있다.(FFNN 신경망처럼 뉴런단위로 RNN을 표현한다면 밑 그림과 같다.)

  <img src="https://user-images.githubusercontent.com/59716219/129435142-1390d3eb-06dd-42c1-92b2-f1ffce5c89cf.png" width="300" height="150">

> ## RNN 수식
  <img src="https://user-images.githubusercontent.com/59716219/129469599-2341e7e6-a007-4745-a3dc-ae9a85ccf517.png" width="300" height="200">

+ <img src="https://render.githubusercontent.com/render/math?math=h_t"> : 현재 시점 t에서의 은닉 상태값
+ <img src="https://render.githubusercontent.com/render/math?math=W_x"> : 입력층에서 입력값을 위한 가중치
+ <img src="https://render.githubusercontent.com/render/math?math=W_h"> : 이전 시점 t-1의 은닉 상태값인 <img src="https://render.githubusercontent.com/render/math?math=h_{t-1}">을 위한 가중치
+ 은닉층 : <img src="https://render.githubusercontent.com/render/math?math=h_t = tanh(W_x x_t + W_h h_{t-1} + b)">
+ 출력층 : <img src="https://render.githubusercontent.com/render/math?math=f(W_y h_t + b)">
  -> 이때 f는 비선형 활성화 함수 중 하나!

> ## RNN벡터의 연산

   <img src="https://user-images.githubusercontent.com/59716219/129469644-243c7f29-d4e7-47a2-90a8-a45eeb94a7b5.png" width="80" height="100"> <img src="https://user-images.githubusercontent.com/59716219/129469646-69219828-292e-4a39-bac2-ca4d3581082c.png" width="400" height="100">

- x : 입력 벡터, d : 단어 벡터의 차원, <img src="https://render.githubusercontent.com/render/math?math=D_h"> : 은닉 상태의 크기

> ## 케라스(Keras)로 RNN 구현하기
> - hidden_size : 은닉상태의 크기를 정의. output_dim과도 동일
> - timesteps : 입력 시퀀스의 길이. 시점의 수
> - input_dim : 입력의 크기
> - RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받는다.
  ```python
  # 은닉층인 RNN층에 대한 코드. return하는 결과값은 은닉상태이다. 출력상태 X
  # return_sequences를 따로 설정해주지 않았으므로 마지막층 은닉 상태 출력
  model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))
  # 다른 표기
  model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))
  # Output Shape : (None, 3)
  
  # batch_size추가
  model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))
  # Output Shape : (8,3)
  
  # return_sequences = True
  model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))
  # Output Shape : (8, 2, 3) 
  ```
> ## Python으로 RNN 구현하기
> <img src="https://render.githubusercontent.com/render/math?math=h = tanh(W_xX_t + W_hh_{t-1} + b)">를 기억하자
  ```python
  import numpy as np

  timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.
  input_dim = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.
  hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.

  inputs = np.random.random((timesteps, input_dim)) # 입력에 해당되는 2D 텐서

  hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화
  # 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬.
  Wx = np.random.random((hidden_size, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.
  Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.
  b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).
  total_hidden_states = []

  # 메모리 셀 동작
  for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.
    output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)
    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적
    print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)
    hidden_state_t = output_t

  total_hidden_states = np.stack(total_hidden_states, axis = 0) 
  # 출력 시 값을 깔끔하게 해준다.

  print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력.
  ```  
  
> ## 깊은 순환 신경망
   <img src="https://user-images.githubusercontent.com/59716219/129470750-60c3bfbe-0292-4cca-98dc-a706e49150fa.png" width="300" height="200">
 
  ```python
  model = Sequential()
  model.add(SimpleRNN(hidden_size, return_sequences = True))
  model.add(SimpleRNN(hidden_size, return_sequences = True))
  ```

> ## 양방향 순환 신경망
> - 단방향 RNN은 과거 시점의 데이터만 참고한다.
> - 양방향 RNN은 향후 시점의 데이터의 힌트를 얻을 수 있다.
> - 양방향 RNN은 하나의 출력값 예측을 위해 2개의 메모리셀을 사용하는 것이 특징
   <img src="https://user-images.githubusercontent.com/59716219/129470810-71d1a5c3-09a7-47e8-8145-0bd2995648bf.png" width="400" height="200">

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import SimpleRNN, Bidirectional

  model = Sequential()
  model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))
  # 2개의 은닉층을 쌓고 싶다면 밑 코드 주석 풀기
  # model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))
  ```
> ### 바닐라 RNN은 시점(time step)이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못한다.-> __장기 의존성 문제!__


---

# 2.LSTM(Long Short-Term Memory,장단기 메모리)
  <img src="https://user-images.githubusercontent.com/59716219/129471738-99b99d40-b1af-4ba4-8f9c-d60c78b2cab7.png" width="300" height="240">
  
- LSTM은 입력게이트, 삭제 게이트, 출력게이트를 가진다
- LSTM은 셀 상태(장기 상태)와 은닉상태(단기 상태)를 출력
- 각 게이트는 공통적으로 시그모이드 함수가 존재하고, 0~1사이의 값으로 게이트를 조절한다.
- 셀 상태는 입력 게이트와 삭제 게이트에 의해 조절되고, 은닉 상태는 출력게이트에 의해 조절된다
- LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보인다!
- <img src="https://render.githubusercontent.com/render/math?math=\sigma">:시그모이드 함수
- <img src="https://render.githubusercontent.com/render/math?math=tanh">:하이퍼볼릭탄젠트 함수
- <img src="https://render.githubusercontent.com/render/math?math=W_{xi},W_{xg},W_{xf},W_{xo}">:<img src="https://render.githubusercontent.com/render/math?math=x_t">와 함께 각 게이트에서 사용되는 4개의 가중치
- <img src="https://render.githubusercontent.com/render/math?math=W_{hi},W_{hg},W_{hf},W_{ho}"> : <img src="https://render.githubusercontent.com/render/math?math=h_{t-1}">와 함께 각 게이트에서 사용되는 4개의 가중치
- <img src="https://render.githubusercontent.com/render/math?math=b_i, b_g, b_f, b_o">는 : 각 게이트에서 사용되는 4개의 편향 


> ## 입력 게이트
  <img src="https://user-images.githubusercontent.com/59716219/129471768-1ddb0d72-3397-46ba-b87c-80948c73a629.png" width="300" height="240">
  
- <img src="https://render.githubusercontent.com/render/math?math=i_t"> : 시그모이드 함수를 지나 0과 1사이의 값을 가진다
- <img src="https://render.githubusercontent.com/render/math?math=g_t"> : 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값을 가진다
- 셀 상태를 구할 때, <img src="https://render.githubusercontent.com/render/math?math=i_t">와 <img src="https://render.githubusercontent.com/render/math?math=g_t">는 원소별 곱을 진행하고, 이 값이 기억할 값이 된다. 

> ## 삭제 게이트
  <img src="https://user-images.githubusercontent.com/59716219/129472070-a5ccdf19-b7e9-42a7-9be3-206f22454315.png" width="300" height="240">

- 기억을 삭제하기 위한 게이트
- <img src="https://render.githubusercontent.com/render/math?math=f_t">은 0~1사이 값을 가지는데, 이 값은 삭제 과정을 거칠 정보의 양이 된다.
- 0에 가까울수록 정보가 많이 삭제된 것이고, 1에 가까울수록 정보를 온전히 기억함


> ## 셀 상태(장기 상태)
  <img src="https://user-images.githubusercontent.com/59716219/129472098-567e3d6f-2459-47cb-9b70-0c13a7e32e07.png" width="300" height="240">

- 먼저 삭제 게이트에서 일부 기억을 잃은 상태가 된다
- 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더한다. 그 후 다음 시점의 LSTM셀로 넘어가게 됨
- 만약 삭제게이트의 출력값이 0이 된다면, 이전 시점의 셀 상태값은 현재 시점의 셀 상태값에 영향력이 0이 된다. 오직 입력게이트의 결과만이 현재 시점의 셀 상태값을 결정
- 반대로 입력 게이트의 <img src="https://render.githubusercontent.com/render/math?math=i_t">의 값이 0이 된다면, 현재 시점의 셀 상태값은 이전 시점의 셀 상태값에만 의존하게 된다. 
- __삭제 게이트 : 이전 시점의 입력을 얼마나 반영할지를 의미, 입력 게이트 : 현재 시점의 입력을 얼마나 반영할지 결정__


> ## 출력 게이트와 은닉 상태(단기 상태)
  <img src="https://user-images.githubusercontent.com/59716219/129472107-fad23ee6-fdab-4377-930f-a61de8bce546.png" width="300" height="240">

- 출력게이트는 현재시점 t의 x값과 이전시점 t-1의 은닉상태가 시그모이드 함수를 지난 값이다. 이 값은 현재 시점 t의 은닉 상태를 결정하는 일에 쓰이게 된다.
- 장기 상태의 값이 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값이 되고, 이 값은 출력 게이트의 값과 연산되며 값이 걸러지는 효과가 발생한다. -> 은닉 상태, 단기 상태 
- 단기 상태의 값은 또한 출력층으로도 향함

---

# 3.GRU(Gated Recurrent Unit,게이트 순환 유닛)

  <img src="https://user-images.githubusercontent.com/59716219/129477463-5dc7135d-ccae-4d64-beff-5af84cac5f12.png" width="500" height="240">

- GRU의 성능은 LSTM과 유사하지만, 복잡한 LSTM의 구조를 간단화 시킴
- 데이터의 양이 적을 때는 매개변수의 양이 적은 GRU가 좀 더 낫고, 데이터의 양이 더 많으면 LSTM이 더 낫다.
- GRU는 업데이트 게이트와 리셋 게이트 두가지만 존재
- 리셋 게이트 : 이전 상태를 얼마나 반영할지 나타냄 
- 업데이트 게이트 : 이전상태와 현재 상태를 얼마만큼의 비률로 반영할지 나타냄

> ## 리셋 게이트

- <img src="https://render.githubusercontent.com/render/math?math=r_t = \sigma({W_r \dot [h_{t-1}, x_t]})">
- 이전 시점의 은닉상태(h)와 현재 입력을 시그모이드에 적용
- 이전 은닉 상태의 값을 얼마나 활용할지를 나타냄
- <img src="https://render.githubusercontent.com/render/math?math=\tilde{h_t}">로 새로운 값을 만든 후 이를 현재 hidden state에 반영한다.

> ## 업데이트 게이트

- 과거와 현재의 정보를 각각 얼마나 반영할지에 대한 비율을 구하는것이 핵심(LSTM의 입력, 삭제게이트와 비슷한 역할)
- <img src="https://render.githubusercontent.com/render/math?math=r_t = z_t">는 현재 정보를 얼마나 사용할지 반영한다.
- <img src="https://render.githubusercontent.com/render/math?math=r_t = 1-z">는 과거 정보에 대해 얼마나 사용할지 반영한다.
- 위 두개를 이용해 현 시점의 출력값을 구할 수 있다. 

---

# 4. SimpleRNN, LSTM with Keras

> ## SimpleRNN
> - 인자 return_sequences(default : False)
>   - False인 경우 : 마지막 시점의 은닉 상태만 출력 
>   - True인 경우 : 모든 은닉층의 상태 출력
> - 인자 return_state(default : False)
>   - False인 경우 : 두개의 출력 리턴
>   - True인 경우 : return_sequences의 여부와 상관없이 마지막 시점의 은닉 상태 출력

  ```python
  # 입력 데이터의 shape : (1, 4, 5)
  
  rnn = SimpleRNN(3)
  hidden_state = rnn(train_X)
  # hidden_states.shape : (1,3)
  
  # return_sequences : True
  rnn = SimpleRNN(3, return_sequences=True)
  hidden_states = rnn(train_X)
  # hidden_states.shape : (1,4,3)
  
  # return_sequences : True, return_state : True
  rnn = SimpleRNN(3, return_sequences=True, return_state=True)
  hidden_states, last_state = rnn(train_X)
  # hidden_states.shape : (1, 4, 3)
  # last_state.shape : (1,3)
  
  # return_sequences : False, return_state : True
  rnn = SimpleRNN(3, return_sequences=False, return_state=True)
  hidden_state, last_state = rnn(train_X)
  # hidden_state.shape : (1,3)
  # last_state.shape : (1,3)  
  ```
  
> ## LSTM
> - 인자 return_sequences(default : False)
>   - False인 경우 : 마지막 시점의 은닉 상태만 출력 
>   - True인 경우 : 모든 은닉층의 상태 출력
> - 인자 return_state(default : False)
>   - False인 경우 : hidden state만 출력
>   - True인 경우 : 셀 상태 반환

```python
lstm = LSTM(3, return_sequences=False, return_state=True)
hidden_state, last_state, last_cell_state = lstm(train_X)
# hidden_state shape: (1, 3)
# last_state shape: (1, 3)
# last_cell_state shape: (1, 3)
# 이때 hidden_state와 last_state는 같다!

lstm = LSTM(3, return_sequences=True, return_state=True)
hidden_states, last_hidden_state, last_cell_state = lstm(train_X)
# hidden_states shape: (1, 4, 3)
# last_hidden_state shape: (1, 3)
# last_cell_state shape: (1, 3)

```

> ## Bidirectional(LSTM)
> - 인자 return_sequences(default : False)
>   - False인 경우 : 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태가 연결된 채 반환됨
>   - True인 경우 : 역방향 LSTM의 첫번째 시점의 은닉 상태는 정방향 LSTM의 첫번째 시점의 은닉 상태와 연결
> - 인자 return_state(default : False)
>   - False인 경우 : hidden state만 출력
>   - True인 경우 : 정방향 LSTM의 은닉 상태와 셀 상태, 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환

- return_sequences가 False이고, return_state가 True인 경우

  <img src="https://user-images.githubusercontent.com/59716219/129474962-997d12c4-1736-4f19-8960-f8178d1a8d44.png" width="200" height="140">
  

```python
bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \
                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))
hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)

# hidden states : [[0.6303139  0.6303139  0.6303139  0.70387346 0.70387346 0.70387346]], shape: (1, 6)
# forward_h(정방향 LSTM의 마지막 시점의 은닉 상태값) : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)
# backward_h(역방향 LSTM의 첫번째 시점의 은닉 상태값) : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)
```

- return_sequences가 True이고, return_state가 True인 경우

  <img src="https://user-images.githubusercontent.com/59716219/129474997-c9c60bc3-ed8f-4cd2-be35-970e435283fc.png" width="200" height="140">


```python
bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \
                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))
hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)

# hidden states : [[[0.3590648  0.3590648  0.3590648  **0.70387346 0.70387346 0.70387346**]
#  [0.5511133  0.5511133  0.5511133  0.5886358  0.5886358  0.5886358 ]
#  [0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]
#  [**0.6303139  0.6303139  0.6303139**  0.21942243 0.21942243 0.21942243]]], shape: (1, 4, 6)

# forward_h : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)
# backward_h : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)
```

---

# 5.RNN 언어 모델
> #### RNN은 입력의 길이를 고정하지 않아도 사용할 수 있다!(n-gram은 고정해야함)

> ## RNNLM의 예측
  <img src="https://user-images.githubusercontent.com/59716219/129475234-e290791e-087b-4fa2-9bef-508f1c5c992b.png" width="250" height="170">
  
- 이전 시점의 출력을 현재 시점의 입력으로 한다.(ex) will을 예측하면 will은 다음 시점의 입력이 되어 the를 예측하게 됨
- 결과적으로 세번째 시점에서 fat은 앞서 나온 what, will, the라는 시퀀스로 인해 결정된 단어가 된다.
- 그러나 위 과정은 훈련이 끝난 모델의 테스트 과정동안(실제 사용할 때)일어나는 일

> ## teacher forcing(교사 강요)

-  모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않는다. -> 실제 알고있는 정답을 t+1 시점의 입력으로 사용.
-  teacher forcing을 사용하지 않았을 때, 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 된다. 
-  좀 더 빠르고 효과적으로 훈련시킬 수 있다.

> ## RNN언어 모델 학습 과정
  <img src="https://user-images.githubusercontent.com/59716219/129475460-103aa87e-f286-49b5-bdd9-da7ced42d36b.png" width="600" height="200">

- 입력층 : 단어의 one-hot벡터가 들어온다.
- 임베딩층(embedding layer)
  - 단어를 onr-hot벡터에서 dense벡터로 만들어주기 위해 임베딩해주는 벡터
  - 임베딩 행렬은 역전파 과정에서 다른 가중치들과 함께 학습됨.
  - 단어 집합의 크기가 V라고 하고 임베딩 벡터의 크기가 M이면, 임베딩 행렬은 VxM이 된다.
  - <img src="https://render.githubusercontent.com/render/math?math=e_t = lookup(x_t)">
- 은닉층 : <img src="https://render.githubusercontent.com/render/math?math=h_t = tanh(W_x x_t + W_h h_{t-1}) + b">
- 출력층 : <img src="https://render.githubusercontent.com/render/math?math=f(W_y h_t + b)">


---

# 6.RNN을 이용한 텍스트 생성(Text Generation using RNN)

> ## RNN,LSTM 이용한 텍스트 생성

### Data

  <img src="https://user-images.githubusercontent.com/59716219/129475614-31471b43-e191-4ed6-80e6-b91615cf9a3a.png" width="500" height="500">
  
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from tensorflow.keras.utils import to_categorical

text="""경마장에 있는 말이 뛰고 있다\n
그의 말이 법이다\n
가는 말이 고와야 오는 말이 곱다\n"""

t = Tokenizer()
t.fit_on_texts([text])
vocab_size = len(t.word_index) + 1
# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,
# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에
# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 
# "말이" : 1

# 훈련 데이터 생성
sequences = list()
for line in text.split('\n'): # Wn을 기준으로 문장 토큰화
    encoded = t.texts_to_sequences([line])[0] # tokenizing + 정수인코딩
    for i in range(1, len(encoded)):
        sequence = encoded[:i+1]
        sequences.append(sequence) # X+Y임
# sequences[0] = [2, 3]

max_len=max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력, 6
sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') # padding

# X와 Y분리
sequences = np.array(sequences)
X = sequences[:,:-1]
y = sequences[:,-1]

# y one_hot encoding
y = to_categorical(y, num_classes=vocab_size)

```

### Model
```python

# RNN Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, SimpleRNN

model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # 레이블을 분리하였으므로 이제 X의 길이는 5 
# 임베딩 벡터는 10차원을 가진다. 
# 32의 은닉 상태 크기를 가지는 바닐라 RNN사용
model.add(SimpleRNN(32))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=200, verbose=2)

# LSTM Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, LSTM

model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_len-1))
# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1
model.add(LSTM(128))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=200, verbose=2)
```

### Predict
```python
def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수
    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장
    sentence = ''
    for _ in range(n): # n번 반복
        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩
        # 뒤에서 자동으로 maxlen만큼 잘라줌
        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩 
        result = model.predict_classes(encoded, verbose=0)
    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.
        for word, index in t.word_index.items(): 
            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면
                break # 해당 단어가 예측 단어이므로 break
        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경
        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장
    # for문이므로 이 행동을 다시 반복
    sentence = init_word + sentence
    return sentence
    
print(sentence_generation(model, t, '경마장에', 4))
# '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측
# 경마장에 있는 말이 뛰고 있다
```

---

# 7.글자 단위 RNN(Char RNN)

  <img src="https://user-images.githubusercontent.com/59716219/129476128-fcfec8c4-1a33-4fa8-9bb1-4fd1883bd665.png" width="400" height="140">

- 글자 단위를 입, 출력으로 사용하므로 임베딩층(embedding layer)을 사용하지 않는다. 

> ## many to many RNN

`훈련 데이터 : appl -> pple`
`샘플의 길이가 4라면 4개의 입력 글자 시퀀스로 부터 4개의 출력 글자 시퀀스를 예측. 즉, RNN의 time step은 4번`

### Data

```python
import numpy as np
import urllib.request
from tensorflow.keras.utils import to_categorical
urllib.request.urlretrieve("http://www.gutenberg.org/files/11/11-0.txt", filename="11-0.txt")
f = open('11-0.txt', 'rb')
lines=[]
for line in f: # 데이터를 한 줄씩 읽는다.
    line=line.strip() # strip()을 통해 \r, \n을 제거한다.
    line=line.lower() # 소문자화.
    line=line.decode('ascii', 'ignore') # \xe2\x80\x99 등과 같은 바이트 열 제거
    if len(line) > 0:
        lines.append(line)
f.close()

# lines[0] : project gutenbergs alices adventures in wonderland, by lewis carroll
text = ' '.join(lines) # 한줄로 만들기

# 글자 집합 만들기
char_vocab = sorted(list(set(text)))
vocab_size=len(char_vocab)

# predict할 때 글자 변환을 위한 dict # 숫자, 특수문자, 알파벳등 55개로 이뤄짐
char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여
print(char_to_index)
# index -> char
index_to_char={}
for key, value in char_to_index.items():
    index_to_char[value] = key

# 문장의 길이를 60으로 나눈 후 dataset으로 만듦
train_X = []
train_y = []

for i in range(n_samples): # 2,646번 수행
    X_sample = text[i * seq_length: (i + 1) * seq_length]
    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 가져온다.
    X_encoded = [char_to_index[c] for c in X_sample] # 하나의 문장 샘플에 대해서 정수 인코딩
    train_X.append(X_encoded)

    y_sample = text[i * seq_length + 1: (i + 1) * seq_length + 1] # 오른쪽으로 1칸 쉬프트한다.
    y_encoded = [char_to_index[c] for c in y_sample]
    train_y.append(y_encoded)
# train_x[0] : [44, 46, ... , 37, 47]
# train_y[0] : [46, ... , 37, 47, 0]
# dataset_size : (2646, 60)

# one-hot encoding
# 임베딩층이 없기 때문에 train_x, train_y에 대해 원-핫 인코딩
# 차리 후 size : (2646, 60, 55)
train_X = to_categorical(train_X)
train_y = to_categorical(train_y)
```

### Model
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, TimeDistributed

model = Sequential()
model.add(LSTM(256, input_shape=(None, train_X.shape[2]), return_sequences=True))
model.add(LSTM(256, return_sequences=True))
model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(train_X, train_y, epochs=80, verbose=2)

```

### Predict
```python
def sentence_generation(model, length):
    ix = [np.random.randint(vocab_size)] # 글자에 대한 랜덤 인덱스 생성
    y_char = [index_to_char[ix[-1]]] # 랜덤 익덱스로부터 글자 생성
    print(ix[-1],'번 글자',y_char[-1],'로 예측을 시작!')
    X = np.zeros((1, length, vocab_size)) # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성

    for i in range(length):
        X[0][i][ix[-1]] = 1 # X[0][i][예측한 글자의 인덱스] = 1, 즉, 예측 글자를 다음 입력 시퀀스에 추가
        print(index_to_char[ix[-1]], end="")
        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)
        y_char.append(index_to_char[ix[-1]])
    return ('').join(y_char)
    
sentence_generation(model, 100)
# 49 번 글자 u 로 예측을 시작!
ury-men would have done just as well. the twelve jurors were to say in that dide. he went on in a di'

```


> ## many to ont RNN

`훈련 데이터 : appl -> n`
`5개의 입력 글자 시퀀스로부터 다음 글자 시퀀스를 예측. 즉, RNN의 time step은 5번`

### Data

```python
# 위와 너무 동일하고, 길어지기 때문에 전처리 생략
# stude -> n 
# tuden -> t

length = 11
sequences = []
for i in range(length, len(text)):
    seq = text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.
    sequences.append(seq)
# 총 훈련 샘플의 개수 : 426
# sequences[0] : 'I get on wi'

X = []
for line in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.
    temp_X = [char_to_index[char] for char in line] # 문장 샘플에서 각 글자에 대해서 정수 인코딩을 수행.
    X.append(temp_X)
# X[0] : [8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18]

# X와 Y분리
sequences = np.array(X)
X = sequences[:,:-1]
y = sequences[:,-1] # 맨 마지막 위치의 글자를 분리
# X.shape : (426, 10)

# X,Y에 대한 one-hot인코딩 수행
# X.shape : (426, 10, 33)
sequences = [to_categorical(x, num_classes=vocab_size) for x in X] # X에 대한 원-핫 인코딩
X = np.array(sequences)
y = to_categorical(y, num_classes=vocab_size) # y에 대한 원-핫 인코딩
```

### Model
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from tensorflow.keras.preprocessing.sequence import pad_sequences

model = Sequential()
model.add(LSTM(80, input_shape=(X.shape[1], X.shape[2])))
model.add(Dense(vocab_size, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=100, verbose=2)
```

### Predict
```python
def sentence_generation(model, char_to_index, seq_length, seed_text, n):
# 모델, 인덱스 정보, 문장 길이, 초기 시퀀스, 반복 횟수
    init_text = seed_text # 문장 생성에 사용할 초기 시퀀스
    sentence = ''

    for _ in range(n): # n번 반복
        encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩
        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩
        encoded = to_categorical(encoded, num_classes=len(char_to_index))
        result = model.predict_classes(encoded, verbose=0)
        # 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 글자)를 result에 저장.
        for char, index in char_to_index.items(): # 만약 예측한 글자와 인덱스와 동일한 글자가 있다면
            if index == result: # 해당 글자가 예측 글자이므로 break
                break
        seed_text=seed_text + char # 현재 시퀀스 + 예측 글자를 현재 시퀀스로 변경
        sentence=sentence + char # 예측 글자를 문장에 저장
        # for문이므로 이 작업을 다시 반복

    sentence = init_text + sentence
    return sentenc
 print(sentence_generation(model, char_to_index, 10, 'I get on w', 80))
 I get on with life as a programmer, I like to hang out with programming and deep learning.
```
